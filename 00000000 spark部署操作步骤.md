

## 常用网址

- spark: http://nn1-87368:4040
- hdfs : http://nn1-87368:50070/explore.html#/
- 

## 常用命令

```bash

# login in nn1

# 查看各节点jps状况
/root/batch_shell/ssh_all.sh jps

# hdfs创建文件夹
hadoop fs -mkdir /tecwang

# 递归方式创建多层文件夹
hadoop fs -mkdir -p /tecwang/spark3.1.3

# [root@nn1-25208 ~]# hadoop fs -ls /
# Found 4 items
# drwxrwx---   - root supergroup          0 2024-09-09 16:20 /data
# drwxr-xr-x   - root supergroup          0 2024-09-09 16:26 /tecwang
# drwxr-xr-x   - root supergroup          0 2024-09-09 16:24 /tmp
# drwxr-xr-x   - root supergroup          0 2024-09-09 16:24 /user

# 

```

## local模式

申请一个最简单的linux环境

并把spark解压缩至对应文件夹，并进行简单配置即可实现

```bash

# 解压缩 spark 至用户目录
tar -zxvf /public/software/bigdata/spark-3.1.3-bin-hadoop2.7.tgz -C /usr/local/

# 创建软链接
ln -s /usr/local/spark-3.1.3-bin-hadoop2.7/ /usr/local/spark-local

# 安装java
# 安装后JAVA_HOME的环境变量会自动配置
rpm -ivh /public/software/language/java/jdk-8u144-linux-x64.rpm

# 添加环境变量
echo export SPARK_HOME=/usr/local/spark-local >> /etc/profile
# 此处需要增加转义符
echo export PATH=\$PATH:\$SPARK_HOME/bin >> /etc/profile

# 环境变量生效
source /etc/profile

# 此时就可以试着提交一些sql任务了
# /usr/local/spark/bin/spark-sql -e "select 1 as col1,2 as col2, 1+9 as col_tmp"

# 也可以试着执行spark提供的样例
# --master表示用master提供资源
    # local表示单线程执行，local[2]表示多线程执行
    # local[*]表示最大线程数
# /usr/local/spark-local/bin/spark-submit \
# --class org.apache.spark.examples.SparkPi \
# --master local[2] \
# /usr/local/spark-local/examples/jars/spark-examples_2.12-3.1.3.jar \
# 1000

# 交互式提交 spark-sql 任务
spark-sql

```

## yarn模式

需要结合 hadoop2.7.3 环境实施

### 1. 登录nn1，并执行以下命令

```bash
# 解压缩 spark 至用户目录
tar -zxvf /public/software/bigdata/spark-3.1.3-bin-hadoop2.7.tgz -C /usr/local/

# 创建软链接
ln -s /usr/local/spark-3.1.3-bin-hadoop2.7/ /usr/local/spark-yarn

# 修改配置信息
cp /usr/local/spark-yarn/conf/spark-env.sh.template /usr/local/spark-yarn/conf/spark-env.sh

# 添加环境变量
echo export SPARK_HOME=/usr/local/spark-yarn >> /etc/profile
# 此处需要增加转义符
echo export PATH=\$PATH:\$SPARK_HOME/bin >> /etc/profile

# 环境变量生效
source /etc/profile

# 追加配置, 告知spark yarn-site的配置在哪里，以便提交任务到yarn
# /usr/local/hadoop-2.7.3/etc/hadoop/yarn-site.xml
echo \# configure yarn-site.xml path for submit jobs to yarn
echo YARN_CONF_DIR=/usr/local/hadoop-2.7.3/etc/hadoop >> /usr/local/spark-yarn/conf/spark-env.sh

# 调整spark配置文件，支持spark web ui历史记录访问呢
cp /usr/local/spark-yarn/conf/spark-defaults.conf.template /usr/local/spark-yarn/conf/spark-defaults.conf

# 创建日志文件夹
hadoop fs -mkdir /directory

# 追加配置
echo spark.eventLog.enabled true >> /usr/local/spark-yarn/conf/spark-defaults.conf
echo spark.eventLog.dir hdfs://$HOSTNAME:8020/directory >> /usr/local/spark-yarn/conf/spark-defaults.conf
echo spark.yarn.historyServer.address=$HOSTNAME:18080 >> /usr/local/spark-yarn/conf/spark-defaults.conf
echo spark.history.ui.port=18080 >> /usr/local/spark-yarn/conf/spark-defaults.conf

# 追加配置
echo \# enable spark history log
echo export SPARK_HISROTY_OPTS=\" >> /usr/local/spark-yarn/conf/spark-env.sh
echo -Dspark.history.ui.port=18080 >> /usr/local/spark-yarn/conf/spark-env.sh
echo -Dspark.history.fs.logDirectory=hdfs://$HOSTNAME:8020/directory >> /usr/local/spark-yarn/conf/spark-env.sh
echo -Dspark.history.retainedApplications=30\" >> /usr/local/spark-yarn/conf/spark-env.sh

# 重启 hadoop历史服务
/usr/local/spark-yarn/sbin/stop-history-server.sh
/usr/local/spark-yarn/sbin/start-history-server.sh


# 重启 hadoop服务（需要在nn1执行）
/usr/local/hadoop-2.7.3/sbin/stop-dfs.sh
/usr/local/hadoop-2.7.3/sbin/stop-yarn.sh
/usr/local/hadoop-2.7.3/sbin/start-dfs.sh
/usr/local/hadoop-2.7.3/sbin/start-yarn.sh
# 启动代理服务，否则无法跳转 Tracking Url
/usr/local/hadoop-2.7.3/sbin/yarn-daemon.sh start proxyserver

# 提交spark任务
# 1 增加了一些 executors的参数，用于适配海牛的环境，资源太少了。
# 2 指定了任务的提交队列，否则任务会被拒绝
# 3 为什么要设置 executor 的内存为 620MB ?
    # 海牛设置的 yarn 的资源上线为 1024MB, 并且预留了 overhead memory 384MB， 则 1024 - 384 = 640 MB
/usr/local/spark-yarn/bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn \
--num-executors 3 \
--executor-cores 3 \
--driver-memory 620m \
--executor-memory 620MB \
--queue hainiu \
# --verbose \
/usr/local/spark-yarn/examples/jars/spark-examples_2.12-3.1.3.jar \
10000

# ERROR 1: Rejected by queue placement policy
# Error info: 24/09/08 11:16:44 INFO spark.SparkContext: Successfully stopped SparkContext. Exception in thread "main" org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1725765237665_0001 to YARN : Application rejected by queue placement policy
# Solution: 在提交spark任务时，指定目标队列为hainiu即可。

# Error 2: yarn 无法跳转 spark 页面，显示无法连接
# Solution: 启动 proxyserver 服务

```